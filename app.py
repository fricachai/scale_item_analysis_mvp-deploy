# app.py
# -*- coding: utf-8 -*-
import io
import os
import re
import math
import traceback

import numpy as np
import pandas as pd
import streamlit as st
import statsmodels.api as sm
from scipy.stats import norm
from scipy.stats import pearsonr
from scipy.stats import ttest_ind
from scipy.stats import f as f_dist
from statsmodels.stats.stattools import durbin_watson

from analysis import run_item_analysis, normalize_item_columns


# ---- Optional GPT report (if gpt_report.py exists & has generate_gpt_report) ----
GPT_AVAILABLE = False
generate_gpt_report = None
try:
    from gpt_report import generate_gpt_report  # type: ignore
    GPT_AVAILABLE = callable(generate_gpt_report)
except Exception:
    GPT_AVAILABLE = False
    generate_gpt_report = None


# ---- Page ----
st.set_page_config(page_title="fricachai è«–æ–‡çµ±è¨ˆåˆ†æå°ˆæ¥­ç‰ˆ(release 1.0) 2026.01.28 ", layout="wide")

import streamlit as st
import streamlit_authenticator as stauth

def secrets_to_dict(x):
    """æŠŠ st.secrets çš„å·¢ç‹€ Secrets ç‰©ä»¶è½‰æˆç´” Python dict/list/primitive"""
    if hasattr(x, "to_dict"):
        return secrets_to_dict(x.to_dict())
    if isinstance(x, dict):
        return {k: secrets_to_dict(v) for k, v in x.items()}
    if isinstance(x, (list, tuple)):
        return [secrets_to_dict(v) for v in x]
    return x

import copy

# ===== Authentication (Level B) =====
auth_config = copy.deepcopy(dict(st.secrets["auth"]))  # âœ… è®Šæˆå¯å¯«çš„ dict

authenticator = stauth.Authenticate(
    credentials=auth_config["credentials"],            # âœ… ä¸è¦ç”¨ st.secrets ç›´æ¥å¼•ç”¨
    cookie_name=auth_config["cookie_name"],
    cookie_key=auth_config["cookie_key"],
    cookie_expiry_days=auth_config["cookie_expiry_days"],
)

name, authentication_status, username = authenticator.login("ç™»å…¥ç³»çµ±", "main")

if authentication_status is False:
    st.error("å¸³è™Ÿæˆ–å¯†ç¢¼éŒ¯èª¤")
    st.stop()
elif authentication_status is None:
    st.warning("è«‹å…ˆç™»å…¥")
    st.stop()

# âœ… ç™»å…¥æˆåŠŸæ‰æœƒå¾€ä¸‹è·‘
with st.sidebar:
    authenticator.logout("ç™»å‡º", "sidebar")
    st.caption(f"ç™»å…¥è€…ï¼š{name} ({username})")



st.title("ğŸ“Š fricachai è«–æ–‡çµ±è¨ˆåˆ†æå°ˆæ¥­ç‰ˆ(release 1.0) 2026.01.28")


# ---- Helpers ----
def read_csv_safely(uploaded_file) -> pd.DataFrame:
    """
    Robust CSV loader for Streamlit UploadedFile.
    Tries common encodings and handles BOM.
    """
    if uploaded_file is None:
        raise ValueError("å°šæœªä¸Šå‚³ CSV æª”æ¡ˆã€‚")

    raw = uploaded_file.getvalue()
    if raw is None or len(raw) == 0:
        raise ValueError("ä¸Šå‚³çš„æª”æ¡ˆæ˜¯ç©ºçš„ï¼ˆ0 bytesï¼‰ã€‚è«‹ç¢ºèª CSV å…§å®¹æ˜¯å¦å­˜åœ¨ã€‚")

    encodings = ["utf-8-sig", "utf-8", "cp950", "big5", "latin-1"]
    last_err = None
    for enc in encodings:
        try:
            bio = io.BytesIO(raw)
            return pd.read_csv(bio, encoding=enc)
        except Exception as e:
            last_err = e

    raise ValueError(f"è®€å– CSV å¤±æ•—ï¼ˆå·²å˜—è©¦ {encodings}ï¼‰ã€‚æœ€å¾ŒéŒ¯èª¤ï¼š{repr(last_err)}")


def safe_show_exception(e: Exception):
    st.error("ç™¼ç”ŸéŒ¯èª¤ï¼ˆsafeï¼‰")
    st.code(repr(e))
    with st.expander("Tracebackï¼ˆé™¤éŒ¯ç”¨ï¼‰"):
        st.code(traceback.format_exc())


def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    """
    Excel-friendly: UTF-8 with BOM
    """
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


# ===== Item code detection =====
ITEM_CODE_RE = re.compile(r"^[A-Za-z]\d{2,3}(_\d+)?$")


def _find_item_cols(df: pd.DataFrame) -> list[str]:
    cols: list[str] = []
    for c in df.columns:
        s = str(c).strip()
        if ITEM_CODE_RE.match(s):
            cols.append(s)
    return cols


def _dim_letter(code: str) -> str | None:
    m = re.match(r"^([A-Za-z])", str(code))
    return m.group(1).upper() if m else None


def build_dim_means_per_row(df_norm: pd.DataFrame) -> pd.DataFrame:
    """
    ç”¢ç”Ÿé€åˆ—ï¼ˆæ¯ä»½å•å·ä¸€åˆ—ï¼‰çš„æ§‹é¢å¹³å‡ï¼š
    - ä¾é¡Œé …ä»£ç¢¼ç¬¬ä¸€ç¢¼æ±ºå®šæ§‹é¢ï¼ˆA/B/C...ï¼‰
    - æ¯åˆ—å°è©²æ§‹é¢æ‰€æœ‰é¡Œç›®åš mean(axis=1, skipna=True)
    - è¼¸å‡ºä¿ç•™ floatï¼ˆå¾ŒçºŒè¿´æ­¸/tæª¢å®šæ‰ä¸æœƒè¢«å­—ä¸²å¹²æ“¾ï¼‰
    """
    item_cols_all = _find_item_cols(df_norm)
    if not item_cols_all:
        return pd.DataFrame()

    dims = sorted({d for d in (_dim_letter(c) for c in item_cols_all) if d is not None})

    df_item = df_norm[item_cols_all].apply(pd.to_numeric, errors="coerce")

    out = pd.DataFrame(index=df_norm.index)
    for d in dims:
        cols_d = [c for c in item_cols_all if _dim_letter(c) == d]
        mean_series = df_item[cols_d].mean(axis=1, skipna=True)
        out[d] = mean_series  # float
    return out


# =========================
# Formatting helpers (å››ä½å°æ•¸ + é¡¯è‘—æ˜Ÿè™Ÿ)
# =========================
def _sig_stars(p: float) -> str:
    if pd.isna(p):
        return ""
    if p < 0.001:
        return "***"
    if p < 0.01:
        return "**"
    if p < 0.05:
        return "*"
    return ""


def _p_stars(p: float) -> str:
    if pd.isna(p):
        return ""
    if p < 0.001:
        return "***"
    if p < 0.01:
        return "**"
    if p < 0.05:
        return "*"
    return ""


def _format_no_leading_zero(x: float, ndigits: int = 4) -> str:
    """æŠŠ 0.0200 é¡¯ç¤ºæˆ .0200ï¼ˆæ¯”ç…§è«–æ–‡è¡¨æ ¼ï¼‰"""
    if pd.isna(x):
        return ""
    s = f"{x:.{ndigits}f}"
    if s.startswith("0."):
        return s[1:]
    if s.startswith("-0."):
        return "-" + s[2:]
    return s


def _std_beta(params: pd.Series, X: pd.DataFrame, y: pd.Series) -> dict:
    """
    è¨ˆç®—æ¨™æº–åŒ–ä¿‚æ•¸ Betaï¼šBeta = b * sd(x) / sd(y)
    """
    sd_y = y.std(ddof=1)
    sd_x = X.std(ddof=1)
    out = {}
    for v in X.columns:
        if sd_y == 0 or pd.isna(sd_y) or sd_x[v] == 0 or pd.isna(sd_x[v]):
            out[v] = np.nan
        else:
            out[v] = float(params[v]) * float(sd_x[v] / sd_y)
    return out


def _fmt_beta(beta: float, p: float) -> str:
    if pd.isna(beta):
        return ""
    stars = _sig_stars(p)
    return f"{beta:.4f}{stars}"


def _fmt_t(t: float) -> str:
    if pd.isna(t):
        return ""
    return f"{t:.4f}"


# ===== Regression table =====
def build_regression_table(df: pd.DataFrame, iv_vars: list[str], dv_var: str):
    """
    ç”¢ç”Ÿè¿´æ­¸è¡¨ï¼ˆæ¯”ç…§è«–æ–‡è¡¨æ ¼ï¼‰ï¼š
    - æœªæ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆbï¼›æ¬„åä»ç”¨ã€ŒÎ²ä¼°è¨ˆå€¼ã€ä»¥ç¬¦åˆä½ çš„è¡¨é ­ï¼‰
    - æ¨™æº–åŒ–ä¿‚æ•¸ Betaï¼ˆBeta = b * sd(x) / sd(y)ï¼‰
    - tã€é¡¯è‘—æ€§(p)
    - Fã€P(F)ã€RÂ²ã€Adj RÂ²ã€N
    - âœ… æ‰€æœ‰æ•¸å€¼é¡¯ç¤ºåˆ°å°æ•¸ç¬¬ 4 ä½
    """
    if not iv_vars or not dv_var:
        raise ValueError("è«‹å…ˆè¨­å®šè‡ªè®Šæ•¸èˆ‡ä¾è®Šæ•¸ã€‚")

    cols = iv_vars + [dv_var]
    d = df[cols].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    y = d[dv_var].astype(float)
    X = d[iv_vars].astype(float)
    Xc = sm.add_constant(X, has_constant="add")

    model = sm.OLS(y, Xc).fit()

    params = model.params
    tvals = model.tvalues
    pvals = model.pvalues

    sd_y = y.std(ddof=1)
    sd_x = X.std(ddof=1)

    beta_std = {}
    for v in iv_vars:
        if sd_y == 0 or pd.isna(sd_y) or sd_x[v] == 0 or pd.isna(sd_x[v]):
            beta_std[v] = np.nan
        else:
            beta_std[v] = params[v] * (sd_x[v] / sd_y)

    rows = []
    rows.append(
        {
            "è‡ªè®Šé …": "ï¼ˆå¸¸æ•¸ï¼‰",
            "æœªæ¨™æº–åŒ–ä¿‚æ•¸ Î²ä¼°è¨ˆå€¼": f"{params['const']:.4f}",
            "æ¨™æº–åŒ–ä¿‚æ•¸ Beta": "â€”",
            "t": f"{tvals['const']:.4f}{_sig_stars(pvals['const'])}",
            "é¡¯è‘—æ€§": f"{pvals['const']:.4f}",
        }
    )

    for v in iv_vars:
        rows.append(
            {
                "è‡ªè®Šé …": v,
                "æœªæ¨™æº–åŒ–ä¿‚æ•¸ Î²ä¼°è¨ˆå€¼": f"{params[v]:.4f}",
                "æ¨™æº–åŒ–ä¿‚æ•¸ Beta": ("" if pd.isna(beta_std[v]) else f"{beta_std[v]:.4f}"),
                "t": f"{tvals[v]:.4f}{_sig_stars(pvals[v])}",
                "é¡¯è‘—æ€§": f"{pvals[v]:.4f}",
            }
        )

    table_df = pd.DataFrame(rows)

    summary = {
        "F": float(model.fvalue) if model.fvalue is not None else np.nan,
        "P(F)": float(model.f_pvalue) if model.f_pvalue is not None else np.nan,
        "R2": float(model.rsquared),
        "Adj_R2": float(model.rsquared_adj),
        "N": int(model.nobs),
    }
    return table_df, summary


# ===== Mediation analysis (IV -> M -> DV) =====
def _to_num_df(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    return df[cols].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")


def _fit_ols(y: pd.Series, X: pd.DataFrame):
    Xc = sm.add_constant(X, has_constant="add")
    return sm.OLS(y, Xc).fit()


def build_mediation_results(
    df: pd.DataFrame,
    iv: str,
    med: str,
    dv: str,
    n_boot: int = 2000,
    seed: int = 42,
):
    """
    ç”¢å‡ºä¸­ä»‹åˆ†æï¼ˆOLSï¼‰ï¼š
    - è·¯å¾‘ a: M ~ IV
    - è·¯å¾‘ c: DV ~ IV
    - è·¯å¾‘ b & c': DV ~ IV + M
    - indirect = a*b
    - Sobel z / pï¼ˆè¿‘ä¼¼ï¼‰
    - bootstrap CIï¼ˆpercentileï¼‰
    """
    d = _to_num_df(df, [iv, med, dv])
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/M/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    # a path
    m_a = _fit_ols(d[med], d[[iv]])
    a = float(m_a.params[iv])
    se_a = float(m_a.bse[iv])
    p_a = float(m_a.pvalues[iv])

    # c path (total)
    m_c = _fit_ols(d[dv], d[[iv]])
    c = float(m_c.params[iv])
    se_c = float(m_c.bse[iv])
    p_c = float(m_c.pvalues[iv])

    # b and c' path
    m_bc = _fit_ols(d[dv], d[[iv, med]])
    b = float(m_bc.params[med])
    se_b = float(m_bc.bse[med])
    p_b = float(m_bc.pvalues[med])

    c_prime = float(m_bc.params[iv])
    se_cprime = float(m_bc.bse[iv])
    p_cprime = float(m_bc.pvalues[iv])

    indirect = a * b

    # Sobel test (normal approximation)
    sobel_se = math.sqrt((b * b * se_a * se_a) + (a * a * se_b * se_b))
    sobel_z = (indirect / sobel_se) if sobel_se != 0 else float("nan")

    if np.isfinite(sobel_z):
        sobel_p = float(2 * (1 - norm.cdf(abs(sobel_z))))
    else:
        sobel_p = float("nan")

    # Bootstrap CI for indirect
    rng = np.random.default_rng(seed)
    n = len(d)
    inds = []
    for _ in range(int(n_boot)):
        idx = rng.integers(0, n, size=n)
        ds = d.iloc[idx]
        try:
            ma = _fit_ols(ds[med], ds[[iv]])
            mbc = _fit_ols(ds[dv], ds[[iv, med]])
            inds.append(float(ma.params[iv]) * float(mbc.params[med]))
        except Exception:
            continue

    if len(inds) >= 20:
        ci_low, ci_high = np.percentile(inds, [2.5, 97.5])
    else:
        ci_low, ci_high = (np.nan, np.nan)

    paths_df = pd.DataFrame(
        [
            {"è·¯å¾‘": "a (IVâ†’M)", "ä¿‚æ•¸": a, "SE": se_a, "t": float(m_a.tvalues[iv]), "p": p_a},
            {"è·¯å¾‘": "c (IVâ†’DV total)", "ä¿‚æ•¸": c, "SE": se_c, "t": float(m_c.tvalues[iv]), "p": p_c},
            {"è·¯å¾‘": "b (Mâ†’DV | IV)", "ä¿‚æ•¸": b, "SE": se_b, "t": float(m_bc.tvalues[med]), "p": p_b},
            {"è·¯å¾‘": "c' (IVâ†’DV direct | M)", "ä¿‚æ•¸": c_prime, "SE": se_cprime, "t": float(m_bc.tvalues[iv]), "p": p_cprime},
        ]
    )

    effects_df = pd.DataFrame(
        [
            {
                "æ•ˆæœ": "Indirect (a*b)",
                "å€¼": indirect,
                "Sobel z": sobel_z,
                "Sobel p": sobel_p,
                "Boot CI 2.5%": ci_low,
                "Boot CI 97.5%": ci_high,
            }
        ]
    )

    summary = {
        "N": int(n),
        "indirect": float(indirect),
        "sobel_z": float(sobel_z) if np.isfinite(sobel_z) else np.nan,
        "sobel_p": float(sobel_p) if np.isfinite(sobel_p) else np.nan,
        "ci_low": float(ci_low) if np.isfinite(ci_low) else np.nan,
        "ci_high": float(ci_high) if np.isfinite(ci_high) else np.nan,
        "boot_used": int(len(inds)),
    }
    return paths_df, effects_df, summary


def build_mediation_paper_table(df: pd.DataFrame, iv: str, med: str, dv: str):
    """
    ç”¢å‡ºè«–æ–‡å¼ä¸­ä»‹åˆ†æè¿´æ­¸è¡¨ï¼ˆå°æ‡‰ä½ å³é‚Šé‚£å¼µè¡¨ï¼‰ï¼š
    æ¢ä»¶äºŒï¼šDV=med, IV=[iv]
    æ¢ä»¶ä¸€ï¼šDV=dv,  IV=[iv]
    æ¢ä»¶ä¸‰ï¼šDV=dv,  IV=[iv, med]
    âœ… æ‰€æœ‰æ•¸å€¼é¡¯ç¤ºåˆ°å°æ•¸ç¬¬ 4 ä½
    """
    d = df[[iv, med, dv]].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/M/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    # ---- Condition 2: M ~ IV ----
    y2 = d[med].astype(float)
    X2 = d[[iv]].astype(float)
    m2 = _fit_ols(y2, X2)
    beta2 = _std_beta(m2.params, X2, y2)

    # ---- Condition 1: DV ~ IV ----
    y1 = d[dv].astype(float)
    X1 = d[[iv]].astype(float)
    m1 = _fit_ols(y1, X1)
    beta1 = _std_beta(m1.params, X1, y1)

    # ---- Condition 3: DV ~ IV + M ----
    y3 = d[dv].astype(float)
    X3 = d[[iv, med]].astype(float)
    m3 = _fit_ols(y3, X3)
    beta3 = _std_beta(m3.params, X3, y3)

    col_c2_beta = f"{med}ï¼ˆæ¢ä»¶äºŒï¼‰Î²å€¼"
    col_c2_t = f"{med}ï¼ˆæ¢ä»¶äºŒï¼‰tå€¼"
    col_c1_beta = f"{dv}ï¼ˆæ¢ä»¶ä¸€ï¼‰Î²å€¼"
    col_c1_t = f"{dv}ï¼ˆæ¢ä»¶ä¸€ï¼‰tå€¼"
    col_c3_beta = f"{dv}ï¼ˆæ¢ä»¶ä¸‰ï¼‰Î²å€¼"
    col_c3_t = f"{dv}ï¼ˆæ¢ä»¶ä¸‰ï¼‰tå€¼"

    rows = []

    rows.append(
        {
            "è‡ªè®Šé …": iv,
            col_c2_beta: _fmt_beta(beta2.get(iv, np.nan), float(m2.pvalues.get(iv, np.nan))),
            col_c2_t: _fmt_t(float(m2.tvalues.get(iv, np.nan))),
            col_c1_beta: _fmt_beta(beta1.get(iv, np.nan), float(m1.pvalues.get(iv, np.nan))),
            col_c1_t: _fmt_t(float(m1.tvalues.get(iv, np.nan))),
            col_c3_beta: _fmt_beta(beta3.get(iv, np.nan), float(m3.pvalues.get(iv, np.nan))),
            col_c3_t: _fmt_t(float(m3.tvalues.get(iv, np.nan))),
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": med,
            col_c2_beta: "",
            col_c2_t: "",
            col_c1_beta: "",
            col_c1_t: "",
            col_c3_beta: _fmt_beta(beta3.get(med, np.nan), float(m3.pvalues.get(med, np.nan))),
            col_c3_t: _fmt_t(float(m3.tvalues.get(med, np.nan))),
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "RÂ²",
            col_c2_beta: f"{float(m2.rsquared):.4f}",
            col_c2_t: "",
            col_c1_beta: f"{float(m1.rsquared):.4f}",
            col_c1_t: "",
            col_c3_beta: f"{float(m3.rsquared):.4f}",
            col_c3_t: "",
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "Î”RÂ²",
            col_c2_beta: f"{float(m2.rsquared_adj):.4f}",
            col_c2_t: "",
            col_c1_beta: f"{float(m1.rsquared_adj):.4f}",
            col_c1_t: "",
            col_c3_beta: f"{float(m3.rsquared_adj):.4f}",
            col_c3_t: "",
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "F",
            col_c2_beta: f"{float(m2.fvalue):.4f}{_sig_stars(float(m2.f_pvalue))}",
            col_c2_t: "",
            col_c1_beta: f"{float(m1.fvalue):.4f}{_sig_stars(float(m1.f_pvalue))}",
            col_c1_t: "",
            col_c3_beta: f"{float(m3.fvalue):.4f}{_sig_stars(float(m3.f_pvalue))}",
            col_c3_t: "",
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "D-W",
            col_c2_beta: f"{float(durbin_watson(m2.resid)):.4f}",
            col_c2_t: "",
            col_c1_beta: f"{float(durbin_watson(m1.resid)):.4f}",
            col_c1_t: "",
            col_c3_beta: f"{float(durbin_watson(m3.resid)):.4f}",
            col_c3_t: "",
        }
    )

    table_df = pd.DataFrame(rows)
    meta = {"N": int(m3.nobs), "cond1": m1, "cond2": m2, "cond3": m3}
    return table_df, meta


def build_moderation_paper_table(df: pd.DataFrame, iv: str, mod: str, dv: str):
    """
    ç”¢å‡ºè«–æ–‡å¼å¹²æ“¾åˆ†æè¿´æ­¸è¡¨ï¼š
    æ¨¡å‹ä¸€ï¼šDV ~ IV
    æ¨¡å‹äºŒï¼šDV ~ IV + MOD
    æ¨¡å‹ä¸‰ï¼šDV ~ IV + MOD + (IVÃ—MOD)
    âœ… æ‰€æœ‰æ•¸å€¼é¡¯ç¤ºåˆ°å°æ•¸ç¬¬ 4 ä½
    """
    d = df[[iv, mod, dv]].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/MOD/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    inter_name = f"{iv}Ã—{mod}"
    d[inter_name] = d[iv] * d[mod]

    y1 = d[dv].astype(float)
    X1 = d[[iv]].astype(float)
    m1 = _fit_ols(y1, X1)
    beta1 = _std_beta(m1.params, X1, y1)

    y2 = d[dv].astype(float)
    X2 = d[[iv, mod]].astype(float)
    m2 = _fit_ols(y2, X2)
    beta2 = _std_beta(m2.params, X2, y2)

    y3 = d[dv].astype(float)
    X3 = d[[iv, mod, inter_name]].astype(float)
    m3 = _fit_ols(y3, X3)
    beta3 = _std_beta(m3.params, X3, y3)

    col_m1_beta = f"{dv}ï¼ˆæ¨¡å‹ä¸€ï¼‰Î²å€¼"
    col_m1_t = f"{dv}ï¼ˆæ¨¡å‹ä¸€ï¼‰tå€¼"
    col_m2_beta = f"{dv}ï¼ˆæ¨¡å‹äºŒï¼‰Î²å€¼"
    col_m2_t = f"{dv}ï¼ˆæ¨¡å‹äºŒï¼‰tå€¼"
    col_m3_beta = f"{dv}ï¼ˆæ¨¡å‹ä¸‰ï¼‰Î²å€¼"
    col_m3_t = f"{dv}ï¼ˆæ¨¡å‹ä¸‰ï¼‰tå€¼"

    r2_1 = float(m1.rsquared)
    r2_2 = float(m2.rsquared)
    r2_3 = float(m3.rsquared)

    dr2_2 = r2_2 - r2_1
    dr2_3 = r2_3 - r2_2

    rows = []

    rows.append(
        {
            "è‡ªè®Šé …": iv,
            col_m1_beta: _fmt_beta(beta1.get(iv, np.nan), float(m1.pvalues.get(iv, np.nan))),
            col_m1_t: _fmt_t(float(m1.tvalues.get(iv, np.nan))),
            col_m2_beta: _fmt_beta(beta2.get(iv, np.nan), float(m2.pvalues.get(iv, np.nan))),
            col_m2_t: _fmt_t(float(m2.tvalues.get(iv, np.nan))),
            col_m3_beta: _fmt_beta(beta3.get(iv, np.nan), float(m3.pvalues.get(iv, np.nan))),
            col_m3_t: _fmt_t(float(m3.tvalues.get(iv, np.nan))),
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": mod,
            col_m1_beta: "",
            col_m1_t: "",
            col_m2_beta: _fmt_beta(beta2.get(mod, np.nan), float(m2.pvalues.get(mod, np.nan))),
            col_m2_t: _fmt_t(float(m2.tvalues.get(mod, np.nan))),
            col_m3_beta: _fmt_beta(beta3.get(mod, np.nan), float(m3.pvalues.get(mod, np.nan))),
            col_m3_t: _fmt_t(float(m3.tvalues.get(mod, np.nan))),
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": f"{iv}*{mod}",
            col_m1_beta: "",
            col_m1_t: "",
            col_m2_beta: "",
            col_m2_t: "",
            col_m3_beta: _fmt_beta(beta3.get(inter_name, np.nan), float(m3.pvalues.get(inter_name, np.nan))),
            col_m3_t: _fmt_t(float(m3.tvalues.get(inter_name, np.nan))),
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "RÂ²",
            col_m1_beta: f"{r2_1:.4f}",
            col_m1_t: "",
            col_m2_beta: f"{r2_2:.4f}",
            col_m2_t: "",
            col_m3_beta: f"{r2_3:.4f}",
            col_m3_t: "",
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "Î”RÂ²",
            col_m1_beta: "",
            col_m1_t: "",
            col_m2_beta: f"{dr2_2:.4f}",
            col_m2_t: "",
            col_m3_beta: f"{dr2_3:.4f}",
            col_m3_t: "",
        }
    )

    rows.append(
        {
            "è‡ªè®Šé …": "F",
            col_m1_beta: f"{float(m1.fvalue):.4f}{_sig_stars(float(m1.f_pvalue))}",
            col_m1_t: "",
            col_m2_beta: f"{float(m2.fvalue):.4f}{_sig_stars(float(m2.f_pvalue))}",
            col_m2_t: "",
            col_m3_beta: f"{float(m3.fvalue):.4f}{_sig_stars(float(m3.f_pvalue))}",
            col_m3_t: "",
        }
    )

    table_df = pd.DataFrame(rows)
    meta = {"N": int(m3.nobs), "interaction_col": inter_name}
    return table_df, meta


def build_discriminant_validity_table(df_norm: pd.DataFrame, item_df: pd.DataFrame):
    """
    å€åˆ¥æ•ˆåº¦åˆ†æè¡¨ï¼ˆCorrelation Matrix + Cronbach's Î± on diagonalï¼‰
    âœ… é¡¯ç¤ºåˆ°å°æ•¸ç¬¬ 4 ä½
    """
    sub_alpha = (
        item_df.groupby("å­æ§‹é¢")["è©²å­æ§‹é¢æ•´é«” Î±"]
        .first()
        .dropna()
        .to_dict()
    )

    sub_dims = sorted(sub_alpha.keys())

    sub_scores = {}
    for sd in sub_dims:
        cols = [c for c in df_norm.columns if isinstance(c, str) and c.startswith(sd)]
        if cols:
            sub_scores[sd] = (
                df_norm[cols]
                .apply(pd.to_numeric, errors="coerce")
                .mean(axis=1)
            )

    score_df = pd.DataFrame(sub_scores).dropna(axis=0, how="any")

    mat = pd.DataFrame("", index=sub_dims, columns=sub_dims)

    for i, r in enumerate(sub_dims):
        for j, c in enumerate(sub_dims):
            if i == j:
                try:
                    mat.loc[r, c] = f"{float(sub_alpha[r]):.4f}"
                except Exception:
                    mat.loc[r, c] = str(sub_alpha[r])
            elif i > j:
                r_val, p_val = pearsonr(score_df[r], score_df[c])
                star = "**" if p_val < 0.01 else ""
                mat.loc[r, c] = f"{r_val:.4f}{star}"
            else:
                mat.loc[r, c] = ""

    return mat

# =========================
# âœ… æ§‹é¢ç¾æ³åˆ†æè¡¨ï¼ˆé¡Œè™Ÿ/æ§‹é¢/å•é …/å¹³å‡æ•¸/æ¨™æº–å·®/æ§‹é¢æ’åº/æ§‹é¢å¹³å‡ï¼‰
# =========================

def _subdim_code(item_code: str) -> str:
    """
    å­æ§‹é¢ä»£ç¢¼ï¼šå–é¡Œé …ä»£ç¢¼å‰å…©ç¢¼
    ä¾‹å¦‚ï¼šA11â†’A1ã€A105â†’A1ã€D54â†’D5
    """
    s = str(item_code).strip()
    return s[:2].upper() if len(s) >= 2 else s.upper()


def _item_sort_key(code: str):
    """
    é¡Œè™Ÿæ’åºï¼šA11, A12, ... A105, ... D54
    è¦å‰‡ï¼šå…ˆå­—æ¯ï¼Œå†æ•¸å­—ï¼ˆè½‰ intï¼‰ï¼Œå†è™•ç† _ å¾Œç¶´
    """
    s = str(code).strip()
    m = re.match(r"^([A-Za-z])(\d+)(?:_(\d+))?$", s)
    if not m:
        return (s, 10**9, 10**9)
    letter = m.group(1).upper()
    num = int(m.group(2))
    suf = int(m.group(3)) if m.group(3) is not None else 0
    return (letter, num, suf)


def build_item_status_table(df_raw: pd.DataFrame, df_norm: pd.DataFrame, mapping: dict) -> pd.DataFrame:
    """
    ç”¢å‡ºã€Œæ§‹é¢ç¾æ³åˆ†æè¡¨ã€ï¼ˆæ¯”ç…§ä½ ç¬¬äºŒå¼µæˆªåœ–çš„æ ¼å¼ï¼‰
    æ¬„ä½ï¼š
    - é¡Œè™Ÿï¼šA11 ... D54ï¼ˆä¾å¯¦éš›é¡Œæ•¸ï¼‰
    - æ§‹é¢ï¼šå­æ§‹é¢ä»£ç¢¼ï¼ˆA1ã€A2...ï¼‰
    - å•é …ï¼šåŸå§‹æ¬„åï¼ˆè‹¥åŸå§‹æ¬„åæ˜¯å®Œæ•´é¡Œç›®å°±æœƒé¡¯ç¤ºé¡Œç›®æ–‡å­—ï¼‰
    - å¹³å‡æ•¸ã€æ¨™æº–å·®ï¼šé‡å°é¡Œé …æ¬„ä½è¨ˆç®—ï¼ˆå››ä½å°æ•¸ï¼‰
    - æ§‹é¢æ’åºï¼šè©²é¡Œåœ¨å­æ§‹é¢å…§çš„å¹³å‡æ•¸æ’åï¼ˆ1=æœ€é«˜ï¼›æ•´æ•¸ï¼‰
    - æ§‹é¢å¹³å‡ï¼šè©²å­æ§‹é¢æ‰€æœ‰é¡Œé …ä¹‹ã€Œå¹³å‡æ•¸ã€å†å–å¹³å‡ï¼ˆå››ä½å°æ•¸ï¼‰
    """
    item_cols = _find_item_cols(df_norm)
    if not item_cols:
        return pd.DataFrame()

    # code -> åŸå§‹æ¬„åï¼ˆå•é …æ–‡å­—ï¼‰
    inv_map = {}
    if isinstance(mapping, dict) and mapping:
        for k, v in mapping.items():
            vv = str(v).strip()
            if vv not in inv_map:
                inv_map[vv] = str(k)

    rows = []
    for code in item_cols:
        x = pd.to_numeric(df_norm[code], errors="coerce")
        mean_v = float(x.mean()) if x.notna().any() else np.nan
        std_v = float(x.std(ddof=1)) if x.notna().sum() >= 2 else np.nan

        sub = _subdim_code(code)
        q_text = inv_map.get(str(code).strip(), str(code).strip())  # è‹¥æ²’æœ‰ mappingï¼Œå°±é€€å›é¡Œè™Ÿæœ¬èº«

        rows.append({
            "é¡Œè™Ÿ": str(code).strip(),
            "æ§‹é¢": sub,
            "å•é …": q_text,
            "å¹³å‡æ•¸": mean_v,
            "æ¨™æº–å·®": std_v,
        })

    out = pd.DataFrame(rows)

    # å­æ§‹é¢å¹³å‡ï¼šä»¥ã€Œè©²å­æ§‹é¢æ‰€æœ‰é¡Œé …çš„å¹³å‡æ•¸ã€å†å–å¹³å‡ï¼ˆå¸¸è¦‹è¡¨æ ¼å¯«æ³•ï¼‰
    sub_mean_map = (
        out.groupby("æ§‹é¢")["å¹³å‡æ•¸"]
        .mean()
        .to_dict()
    )
    out["æ§‹é¢å¹³å‡"] = out["æ§‹é¢"].map(sub_mean_map)

    # æ§‹é¢æ’åºï¼šå­æ§‹é¢å…§ä¾ã€Œå¹³å‡æ•¸ã€ç”±å¤§åˆ°å°æ’åï¼ˆ1=æœ€é«˜ï¼‰ï¼Œæ•´æ•¸
    out["æ§‹é¢æ’åº"] = (
        out.groupby("æ§‹é¢")["å¹³å‡æ•¸"]
        .rank(method="dense", ascending=False)
        .astype("Int64")
    )

    # é¡Œè™Ÿæ’åº
    out = out.sort_values(by="é¡Œè™Ÿ", key=lambda s: s.map(_item_sort_key)).reset_index(drop=True)

    # æ ¼å¼åŒ–ï¼ˆå››ä½å°æ•¸ï¼›æ§‹é¢æ’åºæ•´æ•¸ï¼‰
    out["å¹³å‡æ•¸"] = out["å¹³å‡æ•¸"].map(lambda v: f"{v:.4f}" if np.isfinite(v) else "")
    out["æ¨™æº–å·®"] = out["æ¨™æº–å·®"].map(lambda v: f"{v:.4f}" if np.isfinite(v) else "")
    out["æ§‹é¢å¹³å‡"] = out["æ§‹é¢å¹³å‡"].map(lambda v: f"{v:.4f}" if np.isfinite(v) else "")
    out["æ§‹é¢æ’åº"] = out["æ§‹é¢æ’åº"].astype(str).replace({"<NA>": ""})

    return out


def _find_profile_cols(df_raw: pd.DataFrame, df_norm: pd.DataFrame) -> list[str]:
    """
    æŠ“ã€Œå€‹äººåŸºæœ¬è³‡æ–™æ¬„ä½ã€ï¼š
    - ä»¥åŸå§‹ df_raw çš„æ¬„åç‚ºæº–ï¼ˆé¡¯ç¤ºæ‰æ˜¯ä½ è¦çš„ä¸­æ–‡ï¼‰
    - æ’é™¤é¡Œé …æ¬„ä½ï¼ˆA11/A105...ï¼‰èˆ‡å·²æ­£è¦åŒ–é¡Œé …æ¬„ä½
    """
    item_cols_norm = set(_find_item_cols(df_norm))

    profile_cols = []
    for c in df_raw.columns:
        s = str(c).strip()
        if ITEM_CODE_RE.match(s):
            continue
        if s in item_cols_norm:
            continue
        profile_cols.append(c)

    return profile_cols


def build_independent_ttest_table(
    df: pd.DataFrame,
    group_col: str,
    dv_cols: list[str],
):
    """
    ç¨ç«‹æ¨£æœ¬ t æª¢å®šè¡¨ï¼ˆStudent's t-test, equal_var=Trueï¼‰
    - å…©çµ„åç¨±å–è‡ª group_col çš„å…©å€‹é¡åˆ¥æ–‡å­—
    - åˆ—ï¼šdv_colsï¼ˆA/B/C...ï¼‰
    - âœ… æ‰€æœ‰æ•¸å€¼ï¼šå°æ•¸ç¬¬ 4 ä½
    - âœ… é¡¯è‘—æ˜Ÿè™Ÿï¼šæ¨™åœ¨ã€Œtå€¼ã€æ¬„ä½ï¼ˆä¾ p å€¼åˆ¤æ–·ï¼‰
    - âœ… På€¼æ¬„ä½ï¼šåªé¡¯ç¤ºæ•¸å€¼ï¼Œä¸åŠ æ˜Ÿè™Ÿ
    """
    d = df[[group_col] + dv_cols].copy()

    # group æ¬„ä½æ¸…ç†
    d[group_col] = d[group_col].astype(str).str.strip()
    d = d.replace({group_col: {"": np.nan, "nan": np.nan, "None": np.nan}})
    d = d.dropna(subset=[group_col])

    groups = [g for g in d[group_col].dropna().unique().tolist() if str(g).strip() != ""]
    if len(groups) != 2:
        raise ValueError(f"æ­¤æ¬„ä½éœ€å‰›å¥½å…©çµ„æ‰å¯åšç¨ç«‹æ¨£æœ¬tæª¢å®šï¼ˆç›®å‰={len(groups)}çµ„ï¼‰ã€‚")

    g1, g2 = groups[0], groups[1]

    rows = []
    for v in dv_cols:
        x1 = pd.to_numeric(d.loc[d[group_col] == g1, v], errors="coerce").dropna()
        x2 = pd.to_numeric(d.loc[d[group_col] == g2, v], errors="coerce").dropna()

        if len(x1) < 2 or len(x2) < 2:
            tval, pval = (np.nan, np.nan)
            m1 = float(x1.mean()) if len(x1) else np.nan
            m2 = float(x2.mean()) if len(x2) else np.nan
        else:
            # Student t-testï¼ˆç­‰è®Šç•°ï¼‰
            ttest = ttest_ind(x1, x2, equal_var=True, nan_policy="omit")
            tval, pval = float(ttest.statistic), float(ttest.pvalue)
            m1, m2 = float(x1.mean()), float(x2.mean())

        # âœ… æ˜Ÿè™Ÿä¾ p åˆ¤æ–·ï¼Œä½†è¦åŠ åœ¨ t å€¼ä¸Š
        t_star = _p_stars(pval) if np.isfinite(pval) else ""

        rows.append(
            {
                "è®Šé …": v,
                str(g1): f"{m1:.4f}" if np.isfinite(m1) else "",
                str(g2): f"{m2:.4f}" if np.isfinite(m2) else "",
                "tå€¼": (_format_no_leading_zero(tval, 4) + t_star) if np.isfinite(tval) else "",
                "På€¼": _format_no_leading_zero(pval, 4) if np.isfinite(pval) else "",
            }
        )

    out = pd.DataFrame(rows)
    meta = {"group1": str(g1), "group2": str(g2)}
    return out, meta

def _p_stars(p: float) -> str:
    if pd.isna(p):
        return ""
    if p < 0.001:
        return "***"
    if p < 0.01:
        return "**"
    if p < 0.05:
        return "*"
    return ""


def _fmt_f_with_stars(F: float, p: float) -> str:
    if not np.isfinite(F):
        return ""
    return f"{F:.4f}{_p_stars(p)}"


def _scheffe_posthoc_pairs(
    group_means: dict[str, float],
    group_ns: dict[str, int],
    msw: float,
    dfb: int,
    dfw: int,
    alpha: float = 0.05,
) -> str:
    """
    ScheffÃ© äº‹å¾Œæ¯”è¼ƒï¼ˆpairwiseï¼‰
    å›å‚³æ ¼å¼ä¾‹å¦‚ï¼š "3>1,2"ã€"2,3>1"ã€"3>1"ï¼›è‹¥ç„¡é¡¯è‘—å‰‡å›å‚³ "â€”"
    è¦å‰‡ï¼šä»¥çµ„åˆ¥é †åº 1..k è¡¨ç¤ºï¼›æ–¹å‘ç”±å¹³å‡æ•¸å¤§å°æ±ºå®šã€‚
    """
    if dfw <= 0 or dfb <= 0 or (not np.isfinite(msw)) or msw <= 0:
        return "â€”"

    labels = list(group_means.keys())  # ä¿æŒ UI é¡¯ç¤ºé †åº
    k = len(labels)
    if k < 3:
        return "â€”"

    # ç”¨æ•¸å­—ç·¨ç¢¼ 1..k
    idx_map = {lab: i + 1 for i, lab in enumerate(labels)}

    # æ”¶é›†é¡¯è‘— pairï¼šç”¨ã€Œå¤§è€… > å°è€…ã€
    sig_pairs = []
    for i in range(k):
        for j in range(i + 1, k):
            li, lj = labels[i], labels[j]
            mi, mj = group_means[li], group_means[lj]
            ni, nj = group_ns[li], group_ns[lj]

            if ni < 2 or nj < 2:
                continue

            diff = mi - mj
            denom = msw * (1.0 / ni + 1.0 / nj)
            if denom <= 0:
                continue

            # ScheffÃ© pairwise F
            F_pair = (diff * diff) / denom / dfb
            p_pair = float(f_dist.sf(F_pair, dfb, dfw))

            if p_pair < alpha:
                # direction: higher mean > lower mean
                if mi > mj:
                    sig_pairs.append((idx_map[li], idx_map[lj]))
                elif mj > mi:
                    sig_pairs.append((idx_map[lj], idx_map[li]))

    if not sig_pairs:
        return "â€”"

    # å°‡çµæœæ•´ç†æˆåƒè«–æ–‡çš„ã€Œ3>1,2ã€å½¢å¼ï¼š
    # å…ˆæŠŠæ¯å€‹ã€Œå‹è€…ã€å°æ‡‰çš„ã€Œæ•—è€…é›†åˆã€å½™ç¸½
    win_to_losers: dict[int, set[int]] = {}
    for w, l in sig_pairs:
        win_to_losers.setdefault(w, set()).add(l)

    # ç”Ÿæˆå­—ä¸²ï¼šä¾å‹è€…ç”±å¤§åˆ°å°æ’ï¼ˆç´”å‘ˆç¾ï¼Œèˆ‡å‡å€¼å¤§å°é€šå¸¸ä¸€è‡´ï¼‰
    parts = []
    for w in sorted(win_to_losers.keys(), reverse=True):
        losers = sorted(win_to_losers[w])
        parts.append(f"{w}>{','.join(map(str, losers))}")

    return "ï¼›".join(parts)


def build_oneway_anova_table(
    df: pd.DataFrame,
    group_col: str,
    dv_cols: list[str],
):
    """
    å–®å› å­è®Šç•°æ•¸åˆ†æè¡¨ï¼ˆOne-way ANOVAï¼‰+ ScheffÃ© post hoc
    - æ¬„ï¼šå„çµ„å¹³å‡æ•¸ï¼ˆæ¬„å=çµ„åˆ¥æ–‡å­—ï¼‰
    - Få€¼ï¼šé¡¯ç¤ºåˆ°å°æ•¸ç¬¬4ä½ + é¡¯è‘—æ˜Ÿè™Ÿï¼ˆä¾ pï¼‰
    - På€¼ï¼šé¡¯ç¤ºåˆ°å°æ•¸ç¬¬4ä½ï¼ˆå¯åŠ å»é¦–0çš„æ ¼å¼ï¼‰
    - Scheffeæ³•ï¼šç”¨ 1..k è¡¨ç¤ºçµ„åˆ¥ï¼Œè¼¸å‡ºå¦‚ 3>1,2
    """
    d = df[[group_col] + dv_cols].copy()

    # group æ¬„ä½æ¸…ç†
    d[group_col] = d[group_col].astype(str).str.strip()
    d = d.replace({group_col: {"": np.nan, "nan": np.nan, "None": np.nan}})
    d = d.dropna(subset=[group_col])

    groups = [g for g in d[group_col].dropna().unique().tolist() if str(g).strip() != ""]
    if len(groups) < 3:
        raise ValueError(f"æ­¤æ¬„ä½éœ€è‡³å°‘ 3 çµ„æ‰å¯åšå–®å› å­è®Šç•°æ•¸åˆ†æï¼ˆç›®å‰={len(groups)}çµ„ï¼‰ã€‚")

    rows = []

    for v in dv_cols:
        # å„çµ„è³‡æ–™
        xs = []
        ns = {}
        means = {}

        for g in groups:
            xg = pd.to_numeric(d.loc[d[group_col] == g, v], errors="coerce").dropna()
            xs.append(xg.values)
            ns[str(g)] = int(len(xg))
            means[str(g)] = float(xg.mean()) if len(xg) else np.nan

        # è‡³å°‘æ¯çµ„è¦æœ‰è³‡æ–™æ‰æœ‰æ„ç¾©
        valid_groups = [g for g in groups if ns[str(g)] >= 2]
        k = len(groups)
        N = sum(ns[str(g)] for g in groups)

        if N <= k or len(valid_groups) < 3:
            # è³‡æ–™ä¸è¶³ï¼šç•™ç©º
            Fv, pv = (np.nan, np.nan)
            msw = np.nan
            dfb, dfw = (k - 1, N - k)
        else:
            # ---- è¨ˆç®— ANOVAï¼šMSB / MSW ----
            # grand mean
            all_vals = []
            for g in groups:
                xg = pd.to_numeric(d.loc[d[group_col] == g, v], errors="coerce").dropna().values
                all_vals.append(xg)
            all_concat = np.concatenate([a for a in all_vals if len(a) > 0])
            grand_mean = float(np.mean(all_concat))

            # SSB, SSW
            ssb = 0.0
            ssw = 0.0
            for g in groups:
                xg = pd.to_numeric(d.loc[d[group_col] == g, v], errors="coerce").dropna().values
                if len(xg) == 0:
                    continue
                mg = float(np.mean(xg))
                ssb += len(xg) * (mg - grand_mean) ** 2
                ssw += float(np.sum((xg - mg) ** 2))

            dfb = k - 1
            dfw = N - k
            msb = ssb / dfb if dfb > 0 else np.nan
            msw = ssw / dfw if dfw > 0 else np.nan

            if np.isfinite(msb) and np.isfinite(msw) and msw > 0:
                Fv = msb / msw
                pv = float(f_dist.sf(Fv, dfb, dfw))
            else:
                Fv, pv = (np.nan, np.nan)

        # ScheffÃ©
        scheffe_txt = _scheffe_posthoc_pairs(
            group_means=means,
            group_ns=ns,
            msw=msw,
            dfb=dfb,
            dfw=dfw,
            alpha=0.05,
        )

        row = {"è®Šé …": v}
        # å„çµ„å¹³å‡æ•¸æ¬„ï¼ˆé¡¯ç¤ºå››ä½ï¼‰
        for g in groups:
            m = means.get(str(g), np.nan)
            row[str(g)] = f"{m:.4f}" if np.isfinite(m) else ""

        # F / P / Scheffe
        row["Få€¼"] = _fmt_f_with_stars(float(Fv), float(pv)) if np.isfinite(Fv) else ""
        row["På€¼"] = _format_no_leading_zero(float(pv), 4) if np.isfinite(pv) else ""
        row["Scheffeæ³•"] = scheffe_txt

        rows.append(row)

    out = pd.DataFrame(rows)

    # çµ„åˆ¥ç·¨ç¢¼èªªæ˜ï¼ˆ1..kï¼‰
    code_map = {i + 1: str(g) for i, g in enumerate(groups)}
    meta = {"groups": groups, "code_map": code_map}
    return out, meta


# ---- Sidebar ----
with st.sidebar:
    st.header("è¨­å®š")
    st.caption("1) ä¸Šå‚³ CSV â†’ 2) ç”¢å‡º Item Analysis â†’ 3) ä¸‹è¼‰çµæœï¼ˆCSVï¼‰")

    uploaded_file = st.file_uploader("ä¸Šå‚³ CSV", type=["csv"])

    st.divider()
    st.subheader("GPT è«–æ–‡å ±å‘Šç”Ÿæˆï¼ˆå¯é¸ï¼‰")

    gpt_on = st.toggle("å•Ÿç”¨ GPT å ±å‘Š", value=False, help="éœ€è¦ OpenAI API Key èˆ‡å¯ç”¨é¡åº¦ï¼ˆquotaï¼‰ã€‚")

    model_options = ["gpt-4o-mini", "gpt-4.1-mini", "gpt-4o", "gpt-4.1"]
    model_pick = st.selectbox("é¸æ“‡ GPT æ¨¡å‹", options=model_options, index=0)
    model_custom = st.text_input("æˆ–è‡ªè¡Œè¼¸å…¥æ¨¡å‹åç¨±ï¼ˆé¸å¡«ï¼‰", value="", placeholder="ä¾‹å¦‚ï¼šgpt-4o-mini")
    model_name = (model_custom.strip() or model_pick).strip()

    api_key = st.text_input("OpenAI API Keyï¼ˆä»¥ sk- é–‹é ­ï¼‰", type="password", value="")
    st.caption("å»ºè­°ç”¨ç’°å¢ƒè®Šæ•¸ä¹Ÿå¯ï¼šå…ˆåœ¨ç³»çµ±è¨­å®š OPENAI_API_KEYï¼Œå†ç•™ç©ºæ­¤æ¬„ã€‚")

    st.divider()
    st.subheader("å­æ§‹é¢è¦å‰‡ï¼ˆä½ æŒ‡å®šï¼‰")
    st.write("å­æ§‹é¢åªå–é¡Œé …ä»£ç¢¼çš„**å‰å…©ç¢¼**ï¼šä¾‹å¦‚ A01â†’A0ã€A11â†’A1ã€A105â†’A1")
    st.caption("â€» é€™å€‹è¦å‰‡éœ€ç”± analysis.py çš„åˆ†ç¾¤é‚è¼¯é…åˆï¼ˆè‹¥ä½ å·²æ”¹å¥½ analysis.py å°±æœƒç”Ÿæ•ˆï¼‰ã€‚")


# ---- Main ----
if uploaded_file is None:
    st.info("è«‹å…ˆåœ¨å·¦å´ä¸Šå‚³ CSV æª”æ¡ˆã€‚")
    st.stop()

try:
    df_raw = read_csv_safely(uploaded_file)
except Exception as e:
    safe_show_exception(e)
    st.stop()

df_norm, mapping = normalize_item_columns(df_raw)

st.subheader("åŸå§‹è³‡æ–™é è¦½ï¼ˆå‰ 5 åˆ—ï¼‰")
st.dataframe(df_raw.head(), width="stretch")

with st.expander("æ¬„åæ­£è¦åŒ–å°ç…§ï¼ˆåŸå§‹æ¬„å â†’ é¡Œé …ä»£ç¢¼ï¼‰"):
    if mapping:
        map_df = pd.DataFrame([{"åŸå§‹æ¬„å": k, "é¡Œé …ä»£ç¢¼": v} for k, v in mapping.items()])
        st.dataframe(map_df, width="stretch")
    else:
        st.write("æœªåµæ¸¬åˆ°å¯æ­£è¦åŒ–çš„é¡Œé …æ¬„åï¼ˆè«‹ç¢ºèªæ¬„åæ ¼å¼ï¼‰ã€‚")

st.subheader("ğŸ“ˆ Item Analysis çµæœ")

try:
    # =========================================================
    # 1ï¸âƒ£ Item Analysis
    # =========================================================
    result_df = run_item_analysis(df_norm)
    st.success("Item analysis completed.")
    st.dataframe(result_df, width="stretch", height=520)

    st.download_button(
        "ä¸‹è¼‰ Item Analysis çµæœ CSV",
        data=df_to_csv_bytes(result_df),
        file_name="item_analysis_results.csv",
        mime="text/csv",
    )

    # =========================================================
    # 2ï¸âƒ£ æ§‹é¢é€åˆ—å¹³å‡ï¼ˆåƒ…ä¾›åˆ†æä½¿ç”¨ï¼‰
    # =========================================================
    df_dim_means_row = build_dim_means_per_row(df_norm)
    if df_dim_means_row.empty:
        st.warning("æ‰¾ä¸åˆ°é¡Œé …ä»£ç¢¼æ¬„ä½ï¼Œç„¡æ³•ç”¢ç”Ÿæ§‹é¢å¹³å‡ï¼ˆA/B/C...ï¼‰ã€‚")
        st.stop()

    df_raw_plus_dimmeans = df_norm.copy()
    for c in df_dim_means_row.columns:
        df_raw_plus_dimmeans[c] = df_dim_means_row[c]

    dim_cols = list(df_dim_means_row.columns)

    # =========================================================
    # 3ï¸âƒ£ Discriminant Validityï¼ˆç¨ç«‹ try / exceptï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“Š å€åˆ¥æ•ˆåº¦åˆ†æè¡¨")

    try:
        disc_df = build_discriminant_validity_table(df_norm, result_df)

        st.dataframe(disc_df, width="stretch")
        st.caption("è¨»ï¼šå°è§’ç·šç‚ºå„å­æ§‹é¢ä¹‹ Cronbachâ€™s Î±ï¼›å·¦ä¸‹ä¸‰è§’ç‚ºå­æ§‹é¢é–“ä¹‹çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸ï¼ˆ** P<0.01ï¼‰ã€‚")

        st.download_button(
            "ä¸‹è¼‰ å€åˆ¥æ•ˆåº¦åˆ†æè¡¨ CSV",
            data=df_to_csv_bytes(disc_df),
            file_name="discriminant_validity_table.csv",
            mime="text/csv",
        )

    except Exception as e:
        st.error("å€åˆ¥æ•ˆåº¦åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
        safe_show_exception(e)

    # =========================================================
    # âœ… 3.4ï¸âƒ£ æ§‹é¢ç¾æ³åˆ†æè¡¨ï¼ˆé¡Œè™Ÿ/æ§‹é¢/å•é …/å¹³å‡æ•¸/æ¨™æº–å·®/æ§‹é¢æ’åº/æ§‹é¢å¹³å‡ï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“‹ æ§‹é¢ç¾æ³åˆ†æè¡¨")

    try:
        item_status_df = build_item_status_table(df_raw=df_raw, df_norm=df_norm, mapping=mapping)

        if item_status_df.empty:
            st.info("æ‰¾ä¸åˆ°é¡Œé …ä»£ç¢¼æ¬„ä½ï¼Œç„¡æ³•ç”¢ç”Ÿæ§‹é¢ç¾æ³åˆ†æè¡¨ã€‚")
        else:
            st.dataframe(item_status_df, width="stretch", height=520)

            st.download_button(
                "ä¸‹è¼‰ æ§‹é¢ç¾æ³åˆ†æè¡¨ CSV",
                data=df_to_csv_bytes(item_status_df),
                file_name="item_status_table.csv",
                mime="text/csv",
            )

    except Exception as e:
        st.error("æ§‹é¢ç¾æ³åˆ†æè¡¨å¤±æ•—ï¼ˆsafeï¼‰")
        safe_show_exception(e)



    # =========================================================
    # âœ… 3.5ï¸âƒ£ Independent Samples t-testï¼ˆåŸºæœ¬è³‡æ–™ â†’ æ§‹é¢A/B/C...ï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“Š ç¨ç«‹æ¨£æœ¬ t æª¢å®šï¼ˆåŸºæœ¬è³‡æ–™åˆ†çµ„ï¼‰")

    try:
        profile_cols = _find_profile_cols(df_raw, df_norm)

        if not profile_cols:
            st.info("æœªåµæ¸¬åˆ°å¯ç”¨çš„å€‹äººåŸºæœ¬è³‡æ–™æ¬„ä½ï¼ˆéé¡Œé …æ¬„ä½ï¼‰ã€‚")
        else:
            picked_profiles = st.multiselect(
                "è«‹å‹¾é¸è¦é€²è¡Œç¨ç«‹æ¨£æœ¬tæª¢å®šçš„å€‹äººåŸºæœ¬è³‡æ–™æ¬„ä½ï¼ˆå¯è¤‡é¸ï¼›æ¯å€‹æ¬„ä½éœ€å‰›å¥½å…©çµ„ï¼‰",
                options=profile_cols,
                default=[],
            )

            if picked_profiles:
                dim_cols_for_t = list(df_dim_means_row.columns)

                df_for_t = df_raw.copy()
                for c in df_dim_means_row.columns:
                    df_for_t[c] = df_dim_means_row[c]

                for gc in picked_profiles:
                    st.markdown(f"### {gc} ç¨ç«‹æ¨£æœ¬tæª¢å®šè¡¨")

                    try:
                        t_table, meta = build_independent_ttest_table(
                            df_for_t,
                            group_col=gc,
                            dv_cols=dim_cols_for_t,
                        )

                        st.dataframe(t_table, width="stretch")
                        # âœ… ä½ æŒ‡å®šï¼šè¡¨æ ¼æœ€ä¸‹æ–¹è¨»è§£
                        st.caption("è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001")

                        st.download_button(
                            f"ä¸‹è¼‰ {gc} tæª¢å®šè¡¨ CSV",
                            data=df_to_csv_bytes(t_table),
                            file_name=f"ttest_{str(gc).strip()}.csv",
                            mime="text/csv",
                        )

                    except Exception as e:
                        st.error(f"ã€{gc}ã€‘ç„¡æ³•ç”¢ç”Ÿ t æª¢å®šè¡¨ï¼š{repr(e)}")

            else:
                st.info("è«‹å…ˆå‹¾é¸è‡³å°‘ä¸€å€‹åŸºæœ¬è³‡æ–™æ¬„ä½ã€‚")

    except Exception as e:
        st.error("t æª¢å®šå€å¡Šå¤±æ•—ï¼ˆsafeï¼‰")
        safe_show_exception(e)

     # =========================================================
    # âœ… 3.6ï¸âƒ£ One-way ANOVAï¼ˆåŸºæœ¬è³‡æ–™ â†’ æ§‹é¢A/B/C...ï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“Š å–®å› å­è®Šç•°æ•¸åˆ†æï¼ˆåŸºæœ¬è³‡æ–™åˆ†çµ„ï¼‰")

    try:
        profile_cols2 = _find_profile_cols(df_raw, df_norm)

        if not profile_cols2:
            st.info("æœªåµæ¸¬åˆ°å¯ç”¨çš„å€‹äººåŸºæœ¬è³‡æ–™æ¬„ä½ï¼ˆéé¡Œé …æ¬„ä½ï¼‰ã€‚")
        else:
            picked_profiles_anova = st.multiselect(
                "è«‹å‹¾é¸è¦é€²è¡Œå–®å› å­è®Šç•°æ•¸åˆ†æçš„å€‹äººåŸºæœ¬è³‡æ–™æ¬„ä½ï¼ˆéœ€è‡³å°‘ä¸‰çµ„ï¼‰",
                options=profile_cols2,
                default=[],
                key="anova_profiles",
            )

            if picked_profiles_anova:
                dim_cols_for_a = list(df_dim_means_row.columns)

                df_for_a = df_raw.copy()
                for c in df_dim_means_row.columns:
                    df_for_a[c] = df_dim_means_row[c]

                for gc in picked_profiles_anova:
                    st.markdown(f"### {gc} å–®å› å­è®Šç•°æ•¸åˆ†æè¡¨")

                    try:
                        a_table, meta = build_oneway_anova_table(
                            df_for_a,
                            group_col=gc,
                            dv_cols=dim_cols_for_a,
                        )

                        st.dataframe(a_table, width="stretch")

                        # è¨»è§£ï¼šæ˜Ÿè™Ÿè¦å‰‡
                        st.caption("è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001")

                        # Scheffe çµ„åˆ¥ç·¨ç¢¼å°ç…§ï¼ˆ1..kï¼‰
                        code_map = meta.get("code_map", {})
                        if code_map:
                            mapping_txt = "ï¼›".join([f"{k}={v}" for k, v in code_map.items()])
                            st.caption(f"Scheffeæ³•çµ„åˆ¥ä»£ç¢¼ï¼š{mapping_txt}")

                        st.download_button(
                            f"ä¸‹è¼‰ {gc} å–®å› å­è®Šç•°æ•¸åˆ†æè¡¨ CSV",
                            data=df_to_csv_bytes(a_table),
                            file_name=f"anova_{str(gc).strip()}.csv",
                            mime="text/csv",
                        )

                    except Exception as e:
                        st.error(f"ã€{gc}ã€‘ç„¡æ³•ç”¢ç”Ÿå–®å› å­è®Šç•°æ•¸åˆ†æè¡¨ï¼š{repr(e)}")

            else:
                st.info("è«‹å…ˆå‹¾é¸è‡³å°‘ä¸€å€‹åŸºæœ¬è³‡æ–™æ¬„ä½ã€‚")

    except Exception as e:
        st.error("ANOVA å€å¡Šå¤±æ•—ï¼ˆsafeï¼‰")
        safe_show_exception(e)

    # =========================================================
    # 4ï¸âƒ£ ç ”ç©¶è®Šæ•¸è¨­å®šï¼ˆIV / DVï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“Œ ç ”ç©¶è®Šæ•¸è¨­å®šï¼ˆè‡ªè®Šæ•¸ / ä¾è®Šæ•¸ï¼‰")

    iv_vars = st.multiselect("â‘  å‹¾é¸è‡ªè®Šæ•¸ï¼ˆå¯è¤‡é¸ï¼‰", options=dim_cols, default=[])

    dv_var = st.selectbox("â‘¡ é¸æ“‡ä¾è®Šæ•¸ï¼ˆå–®ä¸€ï¼‰", options=[""] + dim_cols, index=0)

    if dv_var and dv_var in iv_vars:
        st.error("âš ï¸ ä¾è®Šæ•¸ä¸å¯åŒæ™‚è¢«é¸ç‚ºè‡ªè®Šæ•¸ï¼Œè«‹é‡æ–°è¨­å®šã€‚")

    elif iv_vars and dv_var:
        st.success(f"ç ”ç©¶æ¨¡å‹ï¼šIV = {iv_vars} â†’ DV = {dv_var}")

        df_research = df_raw_plus_dimmeans[iv_vars + [dv_var]].copy()
        st.dataframe(df_research, width="stretch")

        st.download_button(
            "ä¸‹è¼‰ ç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + DVï¼‰",
            data=df_to_csv_bytes(df_research),
            file_name="research_dataset_IV_DV.csv",
            mime="text/csv",
        )

        # =====================================================
        # 5ï¸âƒ£ Regression
        # =====================================================
        st.divider()
        st.subheader("ğŸ“Š è¿´æ­¸åˆ†æè¡¨ï¼ˆè«–æ–‡æ ¼å¼ï¼‰")

        if st.button("åŸ·è¡Œè¿´æ­¸åˆ†æ", type="primary"):
            try:
                reg_table, reg_sum = build_regression_table(df_research, iv_vars, dv_var)

                st.dataframe(reg_table, width="stretch")
                st.markdown(
                    f"**F={reg_sum['F']:.4f}ï¼ŒP={reg_sum['P(F)']:.4f}ï¼Œ"
                    f"RÂ²={reg_sum['R2']:.4f}ï¼ŒAdj RÂ²={reg_sum['Adj_R2']:.4f}ï¼Œ"
                    f"N={reg_sum['N']}**"
                )

            except Exception as e:
                st.error("è¿´æ­¸åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
                safe_show_exception(e)

    else:
        st.info("è«‹å…ˆé¸æ“‡è‡³å°‘ä¸€å€‹è‡ªè®Šæ•¸èˆ‡ä¸€å€‹ä¾è®Šæ•¸ã€‚")

except Exception as e:
    st.error("Item Analysis ä¸»æµç¨‹å¤±æ•—ï¼ˆsafeï¼‰")
    safe_show_exception(e)
    st.stop()

# ====== Mediation Settings (äº’æ–¥ï¼šA/B/C/D... åªèƒ½å‡ºç¾åœ¨ä¸€å€‹ä½ç½®) ======
st.divider()
st.subheader("ğŸ§© ä¸­ä»‹åˆ†æè¨­å®š")

dim_cols_all = dim_cols

col1, col2, col3 = st.columns(3)

with col1:
    iv_m = st.selectbox("â‘  è‡ªè®Šæ•¸ï¼ˆIVï¼‰", options=[""] + dim_cols_all, index=0, key="med_iv")

with col2:
    med_options = [""] + [c for c in dim_cols_all if c != iv_m]
    med_m = st.selectbox("â‘¡ ä¸­ä»‹è®Šæ•¸ï¼ˆMï¼‰", options=med_options, index=0, key="med_m")

with col3:
    dv_options = [""] + [c for c in dim_cols_all if c not in {iv_m, med_m}]
    dv_m = st.selectbox("â‘¢ ä¾è®Šæ•¸ï¼ˆDVï¼‰", options=dv_options, index=0, key="med_dv")

chosen = [x for x in [iv_m, med_m, dv_m] if x]

if len(chosen) != len(set(chosen)):
    st.error("âš ï¸ IV / M / DV ä¸å¯é‡è¤‡ï¼ŒAã€Bã€Cã€Dâ€¦ æ¯å€‹åªèƒ½å‡ºç¾åœ¨ä¸€å€‹è§’è‰²ä¸­ã€‚")

elif iv_m and med_m and dv_m:
    st.success(f"ä¸­ä»‹æ¨¡å‹ï¼š{iv_m} â†’ {med_m} â†’ {dv_m}")

    st.markdown("### ç ”ç©¶ç”¨è³‡æ–™è¡¨ï¼ˆåƒ…ä¿ç•™ IV / M / DVï¼‰")
    df_mediation = df_raw_plus_dimmeans[[iv_m, med_m, dv_m]].copy()
    st.dataframe(df_mediation, width="stretch")

    st.download_button(
        "ä¸‹è¼‰ ä¸­ä»‹åˆ†æç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + M + DVï¼‰",
        data=df_to_csv_bytes(df_mediation),
        file_name=f"mediation_dataset_{iv_m}_{med_m}_{dv_m}.csv",
        mime="text/csv",
    )

    st.markdown("### ä¸­ä»‹åˆ†æ")

    n_boot = st.number_input("Bootstrap æ¬¡æ•¸ï¼ˆå»ºè­° 2000ï¼‰", min_value=200, max_value=20000, value=2000, step=200)

    if st.button("åŸ·è¡Œä¸­ä»‹åˆ†æ", type="primary", key="run_mediation"):
        try:
            paper_table, meta = build_mediation_paper_table(df_raw_plus_dimmeans, iv=iv_m, med=med_m, dv=dv_m)

            st.markdown(f"### ä¸­ä»‹è®Šæ•¸ï¼ˆ{med_m}ï¼‰å° è‡ªè®Šæ•¸ï¼ˆ{iv_m}ï¼‰èˆ‡ ä¾è®Šæ•¸ï¼ˆ{dv_m}ï¼‰ä¹‹ä¸­ä»‹åˆ†æè¡¨")
            st.dataframe(paper_table, width="stretch")

            st.caption("è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001ï¼›Î”RÂ² ç‚ºèª¿æ•´å¾Œ RÂ²ï¼ˆAdj RÂ²ï¼‰ï¼›D-W ç‚º Durbinâ€“Watsonã€‚")

            tag = f"{iv_m}_to_{med_m}_to_{dv_m}".replace(" ", "")
            st.download_button(
                "ä¸‹è¼‰ ä¸­ä»‹åˆ†æè¡¨ CSV",
                data=df_to_csv_bytes(paper_table),
                file_name=f"mediation_table_{tag}.csv",
                mime="text/csv",
            )

            st.markdown(f"**N={meta['N']}**")

        except Exception as e:
            st.error("ä¸­ä»‹åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
            safe_show_exception(e)

else:
    st.info("è«‹ä¾åºé¸æ“‡ IV / M / DVï¼ˆä¸”ä¸‰è€…ä¸å¯é‡è¤‡ï¼‰å¾Œï¼Œæ‰æœƒé¡¯ç¤ºä¸­ä»‹åˆ†æè³‡æ–™èˆ‡çµæœã€‚")


# =========================
# Moderation (IV -> DV moderated by W)
# =========================
st.divider()
st.subheader("ğŸ§© å¹²æ“¾åˆ†æè¨­å®š")

col1, col2, col3 = st.columns(3)

with col1:
    iv_w = st.selectbox("â‘  è‡ªè®Šæ•¸ï¼ˆIVï¼‰", options=[""] + dim_cols, index=0, key="mod_iv")

mod_options = [""] + [c for c in dim_cols if c != iv_w]
with col2:
    w_var = st.selectbox("â‘¡ å¹²æ“¾è®Šæ•¸ï¼ˆWï¼‰", options=mod_options, index=0, key="mod_w")

dv_options2 = [""] + [c for c in dim_cols if c not in {iv_w, w_var}]
with col3:
    dv_w = st.selectbox("â‘¢ ä¾è®Šæ•¸ï¼ˆDVï¼‰", options=dv_options2, index=0, key="mod_dv")

chosen2 = [x for x in [iv_w, w_var, dv_w] if x]
if len(chosen2) != len(set(chosen2)):
    st.error("âš ï¸ IV / W / DV ä¸å¯é‡è¤‡ï¼ŒAã€Bã€Cã€Dâ€¦ æ¯å€‹åªèƒ½å‡ºç¾åœ¨ä¸€å€‹è§’è‰²ä¸­ã€‚")
else:
    if iv_w and w_var and dv_w:
        st.success(f"å¹²æ“¾æ¨¡å‹ï¼š{iv_w} â†’ {dv_w}ï¼ˆW={w_var}ï¼‰")

        st.markdown("### ç ”ç©¶ç”¨è³‡æ–™è¡¨ï¼ˆåƒ…ä¿ç•™ IV / W / DVï¼‰")
        df_moderation = df_raw_plus_dimmeans[[iv_w, w_var, dv_w]].copy()
        st.dataframe(df_moderation, width="stretch")

        st.download_button(
            "ä¸‹è¼‰ å¹²æ“¾åˆ†æç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + W + DVï¼‰",
            data=df_to_csv_bytes(df_moderation),
            file_name=f"moderation_dataset_{iv_w}_{w_var}_{dv_w}.csv",
            mime="text/csv",
        )

        run_mod = st.button("åŸ·è¡Œå¹²æ“¾åˆ†æ", type="primary", key="run_moderation")

        if run_mod:
            try:
                mod_table, mod_meta = build_moderation_paper_table(df_raw_plus_dimmeans, iv=iv_w, mod=w_var, dv=dv_w)

                st.markdown(f"### å¹²æ“¾è®Šæ•¸ï¼ˆ{w_var}ï¼‰å° è‡ªè®Šæ•¸ï¼ˆ{iv_w}ï¼‰èˆ‡ ä¾è®Šæ•¸ï¼ˆ{dv_w}ï¼‰ä¹‹å¹²æ“¾åˆ†æè¡¨")
                st.dataframe(mod_table, width="stretch")
                st.caption("è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001ï¼›Î”RÂ² ç‚º RÂ² è®ŠåŒ–é‡ï¼ˆRÂ² changeï¼‰ã€‚")

                tag2 = f"{iv_w}_x_{w_var}_to_{dv_w}".replace(" ", "")
                st.download_button(
                    "ä¸‹è¼‰ å¹²æ“¾åˆ†æè¡¨ CSV",
                    data=df_to_csv_bytes(mod_table),
                    file_name=f"moderation_table_{tag2}.csv",
                    mime="text/csv",
                )

                st.markdown(f"**N={mod_meta['N']}**")

            except Exception as e:
                st.error("å¹²æ“¾åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
                safe_show_exception(e)

    else:
        st.info("è«‹ä¾åºé¸æ“‡ IV / W / DVï¼ˆä¸”ä¸‰è€…ä¸å¯é‡è¤‡ï¼‰å¾Œï¼Œæ‰æœƒé¡¯ç¤ºå¹²æ“¾åˆ†æè³‡æ–™èˆ‡çµæœã€‚")


# ---- GPT report (optional) ----
st.divider()
st.subheader("ğŸ“ GPT è«–æ–‡å ±å‘Šç”Ÿæˆï¼ˆæ–‡å­—ï¼‰")

if not gpt_on:
    st.info("ä½ ç›®å‰æœªå•Ÿç”¨ GPT å ±å‘Šã€‚è‹¥è¦ç”Ÿæˆè«–æ–‡æ–‡å­—ï¼Œè«‹åœ¨å·¦å´æ‰“é–‹ã€Œå•Ÿç”¨ GPT å ±å‘Šã€ã€‚")
    st.stop()

if not GPT_AVAILABLE:
    st.warning("æ‰¾ä¸åˆ°å¯ç”¨çš„ generate_gpt_reportï¼ˆè«‹ç¢ºèª gpt_report.py ä¸­æœ‰å®šç¾© generate_gpt_reportï¼‰ã€‚")
    st.stop()

key = (api_key or os.getenv("OPENAI_API_KEY") or "").strip()
if not key:
    st.warning("å°šæœªæä¾› OpenAI API Keyã€‚è«‹åœ¨å·¦å´è¼¸å…¥ï¼Œæˆ–è¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEYã€‚")
    st.stop()

gen = st.button("ç”Ÿæˆ GPT å ±å‘Šï¼ˆæ–‡å­—ï¼‰", type="primary")

if gen:
    try:
        report = generate_gpt_report(result_df, model=model_name, api_key=key)

        paper_text = None
        if isinstance(report, dict):
            paper_text = report.get("paper_text") or report.get("text") or report.get("output")
        elif isinstance(report, str):
            paper_text = report

        if not paper_text:
            st.warning("GPT å›å‚³å…§å®¹ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ gpt_report.py çš„å›å‚³æ ¼å¼ã€‚")
        else:
            st.success("GPT å ±å‘Šç”Ÿæˆå®Œæˆã€‚")
            st.text_area("GPT è«–æ–‡å ±å‘Šï¼ˆå¯è¤‡è£½ï¼‰", value=paper_text, height=420)

            st.download_button(
                "ä¸‹è¼‰ GPT å ±å‘Š TXT",
                data=paper_text.encode("utf-8"),
                file_name="gpt_paper_report.txt",
                mime="text/plain",
            )

    except Exception as e:
        msg = repr(e)
        if "insufficient_quota" in msg or "You exceeded your current quota" in msg:
            st.error("GPT report failedï¼šä½ çš„ OpenAI API å¸³è™Ÿç›®å‰æ²’æœ‰å¯ç”¨é¡åº¦ï¼ˆinsufficient_quotaï¼‰ã€‚")
            st.caption("è§£æ³•ï¼šåˆ° OpenAI å¹³å° Billing/Credits åŠ å€¼å¾Œå†è©¦ã€‚")
        else:
            st.error("GPT report failed. See error details below (safe).")
            safe_show_exception(e)
